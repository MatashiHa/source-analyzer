{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68c8c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Загрузка исходных данных\n",
    "with open(\"training_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Формирование датасета\n",
    "dataset = []\n",
    "for i in range(0, len(data), 2):\n",
    "    user_msg = data[i]\n",
    "    assistant_msg = data[i + 1]\n",
    "\n",
    "    entry = {\"messages\": [user_msg, assistant_msg]}\n",
    "    # entry = [data[i], data[i+1]]\n",
    "    dataset.append(entry)\n",
    "\n",
    "# Пример: вывод первых 3 элементов\n",
    "# for example in dataset[:3]:\n",
    "#     print(json.dumps(example, indent=2, ensure_ascii=False))\n",
    "\n",
    "with open(\"chatml_dataset.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in dataset:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf88963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pafin\\projects\\source-analyzer\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 60 examples [00:00, 2860.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Загрузка .jsonl\n",
    "dataset = load_dataset(\"json\", data_files=\"chatml_dataset.jsonl\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6499a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def json_to_chatml(messages):\n",
    "    chatml = []\n",
    "    for message in messages:\n",
    "        role = message[\"role\"]\n",
    "        content = message[\"content\"]\n",
    "\n",
    "        # Если это ассистент и content является JSON-строкой, форматируем его\n",
    "        if role == \"assistant\":\n",
    "            try:\n",
    "                content_json = json.loads(content)\n",
    "                # Красиво форматируем JSON для вывода\n",
    "                formatted_content = json.dumps(content_json, indent=2)\n",
    "            except json.JSONDecodeError:\n",
    "                formatted_content = content\n",
    "        else:\n",
    "            formatted_content = content\n",
    "\n",
    "        chatml.append(f\"<|im_start|>{role}\\n{formatted_content}<|im_end|>\")\n",
    "\n",
    "    return \"\\n\".join(chatml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c17fd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60/60 [00:00<00:00, 1331.54 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def process_dataset(sample):\n",
    "    sample = json_to_chatml(sample[\"messages\"])\n",
    "    return {\"text\": sample}\n",
    "\n",
    "\n",
    "chat_dataset = dataset.map(process_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f92f7372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"<|im_start|>user\\nCategory: positivity\\nText: the weather is good tonight, but im too tired.<|im_end|>\\n<|im_start|>assistant\\n{\\n  \\\"predicted_class\\\": \\\"low\\\",\\n  \\\"class_to_words\\\": {\\n    \\\"high\\\": [\\n      \\\"good\\\"\\n    ],\\n    \\\"medium\\\": [\\n      \\\"tonight\\\"\\n    ],\\n    \\\"low\\\": [\\n      \\\"tired\\\",\\n      \\\"but\\\"\\n    ]\\n  },\\n  \\\"class_to_probabilities\\\": {\\n    \\\"high\\\": 0.25,\\n    \\\"medium\\\": 0.25,\\n    \\\"low\\\": 0.5\\n  }\\n}<|im_end|>\"\n",
      "\"<|im_start|>user\\nCategory: negativity\\nText: I failed the exam and feel terrible.<|im_end|>\\n<|im_start|>assistant\\n{\\n  \\\"predicted_class\\\": \\\"high\\\",\\n  \\\"class_to_words\\\": {\\n    \\\"high\\\": [\\n      \\\"failed\\\",\\n      \\\"terrible\\\"\\n    ],\\n    \\\"medium\\\": [\\n      \\\"exam\\\"\\n    ],\\n    \\\"low\\\": [\\n      \\\"feel\\\"\\n    ]\\n  },\\n  \\\"class_to_probabilities\\\": {\\n    \\\"high\\\": 0.6,\\n    \\\"medium\\\": 0.3,\\n    \\\"low\\\": 0.1\\n  }\\n}<|im_end|>\"\n",
      "\"<|im_start|>user\\nCategory: excitement\\nText: I can't wait for the concert! It's going to be amazing!<|im_end|>\\n<|im_start|>assistant\\n{\\n  \\\"predicted_class\\\": \\\"high\\\",\\n  \\\"class_to_words\\\": {\\n    \\\"high\\\": [\\n      \\\"can't wait\\\",\\n      \\\"amazing\\\"\\n    ],\\n    \\\"medium\\\": [\\n      \\\"concert\\\"\\n    ],\\n    \\\"low\\\": [\\n      \\\"going\\\"\\n    ]\\n  },\\n  \\\"class_to_probabilities\\\": {\\n    \\\"high\\\": 0.7,\\n    \\\"medium\\\": 0.2,\\n    \\\"low\\\": 0.1\\n  }\\n}<|im_end|>\"\n"
     ]
    }
   ],
   "source": [
    "# Пример: вывод первых 3 элементов\n",
    "for example in chat_dataset[:3][\"text\"]:\n",
    "    print(json.dumps(example, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7921b411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 14.09s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    bnb_config = None\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    os.getenv(\"LLM_MODEL_NAME\"),\n",
    "    token=os.getenv(\"HF_TOKEN\"),\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    os.getenv(\"LLM_MODEL_NAME\"),\n",
    "    token=os.getenv(\"HF_TOKEN\"),\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # квантование требует автоматического распределения между cpu и gpu\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.enable_input_require_grads()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "042ee6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765dce80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11534336 || all params: 2236943360 || trainable%: 0.5156293273335272\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"qkv_proj\", \"o_proj\", \"gate_up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "# model.unload()\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e983c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # особенность для GPT2Tokenizer\n",
    "\n",
    "inputs = tokenizer(\n",
    "    chat_dataset[:][\"text\"], return_tensors=\"pt\", padding=True, truncation=True\n",
    ")\n",
    "train_dataset = Dataset.from_dict({\"input_ids\": inputs[\"input_ids\"]})\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=2,  # используем шаги разогрева для улучшения работы оптимизатора adam\n",
    "    max_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    logging_steps=1,\n",
    "    output_dir=\"outputs\",\n",
    "    optim=\"paged_adamw_8bit\",  # позволяет снизить нагрузку на gpu память и ускорить работу\n",
    "    report_to=\"mlflow\",\n",
    "    label_names=[\"input_ids\", \"attention_mask\"],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "154dc1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"You are an AI assistant that returns ONLY JSON answers. If you output anything but JSON you will have FAILED. Follow these rules:\n",
    "        - Output only valid JSON.\n",
    "        - JSON must include:\n",
    "        - Classify only words/phrases that are related to the category.\n",
    "        - predicted_class: the level (\"high\", \"medium\", or \"low\") with the highest probability among all levels.\n",
    "        - class_to_words: a mapping of each level (\"high\", \"medium\", \"low\") to a list of words or phrases from the text.\n",
    "        - class_to_probabilities: a mapping of each level (\"high\", \"medium\", \"low\") to its probability.\n",
    "        - divide the words by their respective class and probability.\n",
    "        - If word is neutral to a category(names, conjunctions etc.), predicted class should be \"low\".\n",
    "        - Use the source language without reinterpretation.\n",
    "        - One word/phrase can only be in one class. Don't repeat same words.\n",
    "        - Do not mention or classify the provided context.\n",
    "        - Avoid responses with any text outside json.\n",
    "        Keep your answer concise.\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"\n",
    "        Category: positivity\n",
    "        Text: the weather is good tonight, but im too tired.\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"\"\"\n",
    "    {\n",
    "      \"predicted_class\": \"medium\"\n",
    "      \"class_to_words\": {\n",
    "        \"high\": [\"good\"],\n",
    "        \"medium\": [\"tonight\", \"\"but\"\"]\n",
    "        \"low\": [\"tired\"],\n",
    "      },\n",
    "      \"class_to_probabilities\": {\n",
    "        \"high\": 0.25,\n",
    "        \"medium\": 0.5,\n",
    "        \"low\": 0.25\n",
    "        }\n",
    "    }\n",
    "    \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "template = \"\"\"\n",
    "    Category: {category}\n",
    "    Text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1581ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pydantic\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "class Request(pydantic.BaseModel):\n",
    "    category: str\n",
    "    text: str\n",
    "\n",
    "\n",
    "def process_input(request, input: Request):\n",
    "    query_template = template.format(category=input.category, text=input.text)\n",
    "    request.append({\"role\": \"user\", \"content\": query_template})\n",
    "\n",
    "\n",
    "class TextGenerator(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, generation_args):\n",
    "        self.generation_args = generation_args\n",
    "\n",
    "    def load_context(self, context):\n",
    "        self.pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=context.artifacts[\"model\"],\n",
    "            tokenizer=context.artifacts[\"tokenizer\"],\n",
    "        )\n",
    "\n",
    "    def predict(self, context, model_input: list[Request]) -> list:\n",
    "        results = []\n",
    "        for input in model_input:\n",
    "            request = rules\n",
    "            process_input(request, input)\n",
    "\n",
    "            results.append(\n",
    "                self.pipeline(request, **self.generation_args)[0][\"generated_text\"]\n",
    "            )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfa6ab48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:c:/mlruns/645640183581576761', creation_time=1745075482500, experiment_id='645640183581576761', last_update_time=1745075482500, lifecycle_stage='active', name='LoRA Fine-tuning', tags={}>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.tracking.fluent._tracking_uri = None\n",
    "mlflow.set_tracking_uri(\"file:///mlruns\")  # Локальная папка\n",
    "mlflow.set_experiment(\"LoRA Fine-tuning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd57eedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "c:\\Users\\pafin\\projects\\source-analyzer\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 01:39, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.005500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.879200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.579200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.471400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.222500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.201900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.133800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.157400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pafin\\projects\\source-analyzer\\.venv\\Lib\\site-packages\\peft\\tuners\\lora\\bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "2025/04/20 20:09:52 INFO mlflow.models.signature: Inferring model signature from type hints\n",
      "2025/04/20 20:09:52 INFO mlflow.models.signature: Failed to infer output type hint, setting output schema to AnyType. Type hint `list` doesn't contain a collection element type. Fix by adding an element type to the collection type definition, e.g. `list[str]` instead of `list`.\n",
      "Downloading artifacts: 100%|██████████| 3/3 [00:20<00:00,  6.76s/it]   \n",
      "Downloading artifacts: 100%|██████████| 6/6 [00:00<00:00, 54.44it/s]  \n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import peft\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# можно настроить под postgresql: mlflow server --backend-store-uri=\"postgres://username@hostname:port/database\" --default-artifact-root=s3://your-bucket --host=0.0.0.0 --port=5000\n",
    "# или через sqlite mlflow server --backend-store-uri sqlite:///mydb.sqlite\n",
    "# mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "# запускать через mlflow ui --backend-store-uri file:///mlruns\n",
    "\n",
    "\n",
    "requirements = [\n",
    "    f\"torch=={torch.__version__}\",\n",
    "    f\"transformers=={transformers.__version__}\",\n",
    "    f\"peft=={peft.__version__}\",\n",
    "]\n",
    "\n",
    "with mlflow.start_run():\n",
    "    trainer.train()\n",
    "    mlflow.log_artifacts(\"outputs\", artifact_path=\"checkpoints\")\n",
    "\n",
    "    # Адаптер из последнего шага обучения\n",
    "    model.load_adapter(\"./outputs/checkpoint-10\", adapter_name=\"lora_adapter_base\")\n",
    "    # Объединяем LoRA с базовой моделью\n",
    "    merged_model = model.merge_and_unload()\n",
    "    # Сохраняем модель\n",
    "    merged_model.save_pretrained(\"./model\")\n",
    "    tokenizer.save_pretrained(\"./tokenizer\")\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 200,\n",
    "        \"return_full_text\": False,\n",
    "        \"do_sample\": False,\n",
    "    }\n",
    "\n",
    "    # mlflow.log_metric(\"PPL\", eval_results[\"eval_loss\"])\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        artifacts={\n",
    "            \"model\": \"./model\",\n",
    "            \"tokenizer\": \"./tokenizer\",\n",
    "        },\n",
    "        python_model=TextGenerator(generation_args),\n",
    "        pip_requirements=requirements,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd4b7c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Request(\n",
    "    category=\"Неопределённость\",\n",
    "    text=\"Неизвестность грядущего пугает людей\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1865b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "\n",
    "# model_uri = \"runs:/e9e2c229df784ab2a10007f290ba8e19/model\"\n",
    "\n",
    "model = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "\n",
    "output = model.predict([input_data])\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8a57f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rouge'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrouge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Rouge\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# import math\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# eval_results = trainer.evaluate()\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# print(\"Perplexity:\", round(math.exp(eval_results[\"eval_loss\"]), 2))\u001b[39;00m\n\u001b[32m      8\u001b[39m rouge = Rouge()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'rouge'"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "# import math\n",
    "\n",
    "# eval_results = trainer.evaluate()\n",
    "# print(\"Perplexity:\", round(math.exp(eval_results[\"eval_loss\"]), 2))\n",
    "\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(output, \"Что ждёт нас в будущем?\", avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5cb6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 14/14 [00:19<00:00,  1.39s/it]  \n",
      "2025/04/20 19:29:38 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n",
      "2025/04/20 19:29:38 INFO mlflow.utils.virtualenv: Creating a new environment in C:\\Users\\pafin\\AppData\\Local\\Temp\\tmpzjc8jy9l\\envs\\virtualenv_envs\\mlflow-2e75ae3718e46febb49eb9d0be69455d33e39c31 with python version 3.13.2 using uv\n",
      "2025/04/20 19:29:38 INFO mlflow.utils.virtualenv: Installing dependencies\n",
      "2025/04/20 19:29:42 WARNING mlflow.utils.virtualenv: Encountered an unexpected error: ShellCommandException(\"Non-zero exit code: 1\\nCommand: ['cmd', '/c', 'C:\\\\\\\\Users\\\\\\\\pafin\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Temp\\\\\\\\tmpzjc8jy9l\\\\\\\\envs\\\\\\\\virtualenv_envs\\\\\\\\mlflow-2e75ae3718e46febb49eb9d0be69455d33e39c31\\\\\\\\Scripts\\\\\\\\activate.bat & uv pip install --prerelease=allow -r requirements.b493f1d856bd4773a5e9f57f55c101c5.txt']\") while creating a virtualenv environment in C:\\Users\\pafin\\AppData\\Local\\Temp\\tmpzjc8jy9l\\envs\\virtualenv_envs\\mlflow-2e75ae3718e46febb49eb9d0be69455d33e39c31, removing the environment directory...\n",
      "2025/04/20 19:29:51 WARNING mlflow.utils.file_utils: Successfully removed C:\\Users\\pafin\\AppData\\Local\\Temp\\tmpzjc8jy9l\\envs\\virtualenv_envs\\mlflow-2e75ae3718e46febb49eb9d0be69455d33e39c31\n"
     ]
    },
    {
     "ename": "ShellCommandException",
     "evalue": "Non-zero exit code: 1\nCommand: ['cmd', '/c', 'C:\\\\Users\\\\pafin\\\\AppData\\\\Local\\\\Temp\\\\tmpzjc8jy9l\\\\envs\\\\virtualenv_envs\\\\mlflow-2e75ae3718e46febb49eb9d0be69455d33e39c31\\\\Scripts\\\\activate.bat & uv pip install --prerelease=allow -r requirements.b493f1d856bd4773a5e9f57f55c101c5.txt']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mShellCommandException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      3\u001b[39m model_uri = \u001b[33m'\u001b[39m\u001b[33mruns:/e9e2c229df784ab2a10007f290ba8e19/model\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Verify the model with the provided input data using the logged dependencies.\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# For more details, refer to:\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# https://mlflow.org/docs/latest/models.html#validate-models-before-deployment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pafin\\projects\\source-analyzer\\.venv\\Lib\\site-packages\\mlflow\\models\\python_api.py:265\u001b[39m, in \u001b[36mpredict\u001b[39m\u001b[34m(model_uri, input_data, input_path, content_type, output_path, env_manager, install_mlflow, pip_requirements_override, extra_envs)\u001b[39m\n\u001b[32m    262\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_path, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    263\u001b[39m             f.write(input_data)\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m         \u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    267\u001b[39m     _predict(input_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pafin\\projects\\source-analyzer\\.venv\\Lib\\site-packages\\mlflow\\models\\python_api.py:243\u001b[39m, in \u001b[36mpredict.<locals>._predict\u001b[39m\u001b[34m(_input_path)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_predict\u001b[39m(_input_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_flavor_backend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43minstall_mlflow\u001b[49m\u001b[43m=\u001b[49m\u001b[43minstall_mlflow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpyfunc_backend_env_root_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_input_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpip_requirements_override\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpip_requirements_override\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_envs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_envs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pafin\\projects\\source-analyzer\\.venv\\Lib\\site-packages\\mlflow\\pyfunc\\backend.py:207\u001b[39m, in \u001b[36mPyFuncBackend.predict\u001b[39m\u001b[34m(self, model_uri, input_path, output_path, content_type, pip_requirements_override, extra_envs)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pip_requirements_override \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._env_manager == em.CONDA:\n\u001b[32m    202\u001b[39m     \u001b[38;5;66;03m# Conda use = instead of == for version pinning\u001b[39;00m\n\u001b[32m    203\u001b[39m     pip_requirements_override = [\n\u001b[32m    204\u001b[39m         pip_req.replace(\u001b[33m\"\u001b[39m\u001b[33m==\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m pip_req \u001b[38;5;129;01min\u001b[39;00m pip_requirements_override\n\u001b[32m    205\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m environment = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_env\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpip_requirements_override\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpip_requirements_override\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_envs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_envs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    214\u001b[39m     environment.execute(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(predict_cmd))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pafin\\projects\\source-analyzer\\.venv\\Lib\\site-packages\\mlflow\\pyfunc\\backend.py:136\u001b[39m, in \u001b[36mPyFuncBackend.prepare_env\u001b[39m\u001b[34m(self, model_uri, capture_output, pip_requirements_override, extra_envs)\u001b[39m\n\u001b[32m    133\u001b[39m     env_root_dir = \u001b[38;5;28mself\u001b[39m._env_root_dir\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._env_manager \u001b[38;5;129;01min\u001b[39;00m {em.VIRTUALENV, em.UV}:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     activate_cmd = \u001b[43m_get_or_create_virtualenv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_env_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv_root_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv_root_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpip_requirements_override\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpip_requirements_override\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_env_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28mself\u001b[39m._environment = Environment(activate_cmd, extra_env=extra_envs)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._env_manager == em.CONDA:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pafin\\projects\\source-analyzer\\.venv\\Lib\\site-packages\\mlflow\\utils\\virtualenv.py:434\u001b[39m, in \u001b[36m_get_or_create_virtualenv\u001b[39m\u001b[34m(local_model_path, env_id, env_root_dir, capture_output, pip_requirements_override, env_manager)\u001b[39m\n\u001b[32m    431\u001b[39m extra_env = _get_virtualenv_extra_env_vars(env_root_dir)\n\u001b[32m    433\u001b[39m \u001b[38;5;66;03m# Create an environment\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_virtualenv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_model_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpython_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpython_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpyenv_root_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpyenv_root_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpip_requirements_override\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpip_requirements_override\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pafin\\projects\\source-analyzer\\.venv\\Lib\\site-packages\\mlflow\\utils\\virtualenv.py:324\u001b[39m, in \u001b[36m_create_virtualenv\u001b[39m\u001b[34m(local_model_path, python_env, env_dir, pyenv_root_dir, env_manager, extra_env, capture_output, pip_requirements_override)\u001b[39m\n\u001b[32m    322\u001b[39m         Path(tmpdir).joinpath(tmp_req_file).write_text(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(deps))\n\u001b[32m    323\u001b[39m         cmd = _join_commands(activate_cmd, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_deps_cmd_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -r \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtmp_req_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m         \u001b[43m_exec_cmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtmpdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pip_requirements_override:\n\u001b[32m    327\u001b[39m     _logger.info(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mInstalling additional dependencies specified by \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    329\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpip_requirements_override: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpip_requirements_override\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    330\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pafin\\projects\\source-analyzer\\.venv\\Lib\\site-packages\\mlflow\\utils\\process.py:141\u001b[39m, in \u001b[36m_exec_cmd\u001b[39m\u001b[34m(cmd, throw_on_error, extra_env, capture_output, synchronous, stream_output, **kwargs)\u001b[39m\n\u001b[32m    134\u001b[39m comp_process = subprocess.CompletedProcess(\n\u001b[32m    135\u001b[39m     process.args,\n\u001b[32m    136\u001b[39m     returncode=returncode,\n\u001b[32m    137\u001b[39m     stdout=stdout,\n\u001b[32m    138\u001b[39m     stderr=stderr,\n\u001b[32m    139\u001b[39m )\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m throw_on_error \u001b[38;5;129;01mand\u001b[39;00m returncode != \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ShellCommandException.from_completed_process(comp_process)\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m comp_process\n",
      "\u001b[31mShellCommandException\u001b[39m: Non-zero exit code: 1\nCommand: ['cmd', '/c', 'C:\\\\Users\\\\pafin\\\\AppData\\\\Local\\\\Temp\\\\tmpzjc8jy9l\\\\envs\\\\virtualenv_envs\\\\mlflow-2e75ae3718e46febb49eb9d0be69455d33e39c31\\\\Scripts\\\\activate.bat & uv pip install --prerelease=allow -r requirements.b493f1d856bd4773a5e9f57f55c101c5.txt']"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "model_uri = \"runs:/e9e2c229df784ab2a10007f290ba8e19/model\"\n",
    "\n",
    "\n",
    "# Verify the model with the provided input data using the logged dependencies.\n",
    "# For more details, refer to:\n",
    "# https://mlflow.org/docs/latest/models.html#validate-models-before-deployment\n",
    "mlflow.models.predict(\n",
    "    model_uri=model_uri,\n",
    "    input_data=[input_data],\n",
    "    env_manager=\"uv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c052e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "logged_model = \"runs:/f7f6514ad4394e32a32a6dca3dcdb9ea/merged_model\"\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "loaded_model.predict(pd.DataFrame(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffba4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получить список всех экспериментов (включая удалённые)\n",
    "all_experiments = mlflow.search_experiments(view_type=mlflow.entities.ViewType.ALL)\n",
    "\n",
    "# Вывести ID и имена\n",
    "for exp in all_experiments:\n",
    "    print(\n",
    "        f\"ID: {exp.experiment_id}, Name: {exp.name}, Lifecycle: {exp.lifecycle_stage}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb91cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.restore_experiment(experiment_id=\"522564765305824673\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564f27c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = mlflow.get_experiment_by_name(\"LoRA Fine-tuning\")\n",
    "if exp:\n",
    "    mlflow.delete_experiment(exp.experiment_id)  # Полное удаление"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
