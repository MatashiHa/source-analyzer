{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c8c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Загрузка исходных данных\n",
    "with open(\"training_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Формирование датасета\n",
    "dataset = []\n",
    "for i in range(0, len(data), 2):\n",
    "    user_msg = data[i]\n",
    "    assistant_msg = data[i + 1]\n",
    "\n",
    "    entry = {\"messages\": [user_msg, assistant_msg]}\n",
    "    # entry = [data[i], data[i+1]]\n",
    "    dataset.append(entry)\n",
    "\n",
    "# Пример: вывод первых 3 элементов\n",
    "# for example in dataset[:3]:\n",
    "#     print(json.dumps(example, indent=2, ensure_ascii=False))\n",
    "\n",
    "with open(\"chatml_dataset.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in dataset:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf88963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Загрузка .jsonl\n",
    "dataset = load_dataset(\"json\", data_files=\"chatml_dataset.jsonl\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6499a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def json_to_chatml(messages):\n",
    "    chatml = []\n",
    "    for message in messages:\n",
    "        role = message[\"role\"]\n",
    "        content = message[\"content\"]\n",
    "\n",
    "        # Если это ассистент и content является JSON-строкой, форматируем его\n",
    "        if role == \"assistant\":\n",
    "            try:\n",
    "                content_json = json.loads(content)\n",
    "                # Красиво форматируем JSON для вывода\n",
    "                formatted_content = json.dumps(content_json, indent=2)\n",
    "            except json.JSONDecodeError:\n",
    "                formatted_content = content\n",
    "        else:\n",
    "            formatted_content = content\n",
    "\n",
    "        chatml.append(f\"<|im_start|>{role}\\n{formatted_content}<|im_end|>\")\n",
    "\n",
    "    return \"\\n\".join(chatml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c17fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(sample):\n",
    "    sample = json_to_chatml(sample[\"messages\"])\n",
    "    return {\"text\": sample}\n",
    "\n",
    "\n",
    "chat_dataset = dataset.map(process_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92f7372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример: вывод первых 3 элементов\n",
    "for example in chat_dataset[:3][\"text\"]:\n",
    "    print(json.dumps(example, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7921b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    bnb_config = None\n",
    "# from models import load_model, load_tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    os.getenv(\"LLM_MODEL_NAME\"),\n",
    "    token=os.getenv(\"HF_TOKEN\"),\n",
    ")\n",
    "print(device)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    os.getenv(\"LLM_MODEL_NAME\"),\n",
    "    token=os.getenv(\"HF_TOKEN\"),\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # квантование требует автоматического распределения между cpu и gpu\n",
    ")\n",
    "# tokenizer = load_tokenizer()\n",
    "# model = load_model()\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.enable_input_require_grads()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042ee6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765dce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    # \"gate_up_proj\", \"down_proj\"\n",
    "    target_modules=[\"qkv_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "# model.unload()\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e983c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# texts = [\"Hello world\", \"How are you?\"]\n",
    "inputs = tokenizer(\n",
    "    chat_dataset[:][\"text\"], return_tensors=\"pt\", padding=True, truncation=True\n",
    ")\n",
    "train_dataset = Dataset.from_dict({\"input_ids\": inputs[\"input_ids\"]})\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=2,\n",
    "    max_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    # fp16=True,\n",
    "    logging_steps=1,\n",
    "    output_dir=\"outputs\",\n",
    "    optim=\"paged_adamw_8bit\",  # позволяет снизить нагрузку на gpu память и ускорить работу\n",
    "    report_to=\"mlflow\",\n",
    "    label_names=[\"input_ids\", \"attention_mask\"],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44efcef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: добавить оценочные данные\n",
    "\n",
    "# import math\n",
    "\n",
    "# eval_results = trainer.evaluate()\n",
    "# print(\"Perplexity:\", round(math.exp(eval_results[\"eval_loss\"]), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa6ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.tracking.fluent._tracking_uri = None\n",
    "mlflow.set_tracking_uri(\"file:///mlruns\")  # Локальная папка\n",
    "mlflow.set_experiment(\"LoRA Fine-tuning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd57eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import peft\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "mlflow.tracking.fluent._tracking_uri = None\n",
    "# можно настроить под postgresql: mlflow server --backend-store-uri=\"postgres://username@hostname:port/database\" --default-artifact-root=s3://your-bucket --host=0.0.0.0 --port=5000\n",
    "# или через sqlite mlflow server --backend-store-uri sqlite:///mydb.sqlite\n",
    "# mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "# запускать через mlflow ui --backend-store-uri file:///mlruns\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "mlflow.set_tracking_uri(\"file:///mlruns\")  # Локальная папка\n",
    "mlflow.set_experiment(\"LoRA Fine-tuning\")\n",
    "\n",
    "requirements = [\n",
    "    f\"torch=={torch.__version__}\",\n",
    "    f\"transformers=={transformers.__version__}\",\n",
    "    f\"peft=={peft.__version__}\",\n",
    "]\n",
    "\n",
    "with mlflow.start_run():\n",
    "    trainer.train()\n",
    "    mlflow.log_artifacts(\"outputs\", artifact_path=\"checkpoints\")\n",
    "\n",
    "    # Адаптер из последнего шага обучения\n",
    "    model.load_adapter(\"./outputs/checkpoint-10\", adapter_name=\"lora_adapter_base\")\n",
    "    # Объединяем LoRA с базовой моделью\n",
    "    merged_model = model.merge_and_unload()\n",
    "    # Сохраняем модель\n",
    "    merged_model.save_pretrained(\"./merged_model\", max_shard_size=\"1GB\")\n",
    "    tokenizer.save_pretrained(\"./merged_model\", max_shard_size=\"1GB\")\n",
    "\n",
    "    mlflow.transformers.log_model(\n",
    "        # transformers_model={\n",
    "        #     \"model\": merged_model,\n",
    "        #     \"tokenizer\": tokenizer,\n",
    "        # },\n",
    "        transformers_model=pipe,\n",
    "        artifact_path=\"merged_model\",\n",
    "        task=\"text-generation\",\n",
    "        pip_requirements=requirements,\n",
    "    )\n",
    "    # peft_model_path = os.path.join(\"outputs\", \"lora_model\")\n",
    "    # merged_model = trainer.model.save_pretrained(peft_model_path)\n",
    "    # сохраняем lora адаптеры\n",
    "    # trainer.model.save_pretrained(\"./lora_adapters\")  # Сохраняем веса LoRA\n",
    "    # tokenizer.save_pretrained(\"./lora_adapters\")  # Сохраняем токенизатор\n",
    "    # Логируем папку с адаптерами в MLflow как артефакт\n",
    "    # mlflow.log_artifacts(\"./lora_adapters\", artifact_path=\"lora_model\")\n",
    "    # Логируем модель в MLflow\n",
    "    # mlflow.transformers.log_model(\n",
    "    #     transformer_model={\n",
    "    #         \"model\": trainer.model,\n",
    "    #         \"tokenizer\": tokenizer,\n",
    "    #     },\n",
    "    #     artifact_path=\"lora_model\",\n",
    "    #     task=\"text-generation\",\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24ce725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "# generation_args = {\n",
    "#     \"max_new_tokens\": 200,\n",
    "#     \"return_full_text\": False,\n",
    "#     \"do_sample\": False,\n",
    "# }\n",
    "\n",
    "# output = pipe([\"Happiness\", \"Дегтярев наградил мать Овечкина почетным знаком Минспорта.\"], **generation_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5cb6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "model_uri = \"runs:/f7f6514ad4394e32a32a6dca3dcdb9ea/merged_model\"\n",
    "\n",
    "# Replace INPUT_EXAMPLE with your own input example to the model\n",
    "# A valid input example is a data instance suitable for pyfunc prediction\n",
    "input_data = \"category: happiness; text: Дегтярев наградил мать Овечкина почетным знаком Минспорта.\"\n",
    "\n",
    "# Verify the model with the provided input data using the logged dependencies.\n",
    "# For more details, refer to:\n",
    "# https://mlflow.org/docs/latest/models.html#validate-models-before-deployment\n",
    "mlflow.models.predict(\n",
    "    model_uri=model_uri,\n",
    "    input_data=input_data,\n",
    "    env_manager=\"uv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c052e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "logged_model = \"runs:/f7f6514ad4394e32a32a6dca3dcdb9ea/merged_model\"\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "# Predict on a Pandas DataFrame.\n",
    "input_data = \"category: happiness; text: Дегтярев наградил мать Овечкина почетным знаком Минспорта.\"\n",
    "\n",
    "loaded_model.predict(pd.DataFrame(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1865b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "\n",
    "# generation_args = {\n",
    "#     \"max_new_tokens\": 200,\n",
    "#     \"return_full_text\": False,\n",
    "#     \"do_sample\": False,\n",
    "# }\n",
    "\n",
    "# output = pipe(context, **generation_args)\n",
    "\n",
    "model_name = \"qlora-analyzer-base\"\n",
    "model_version = 2\n",
    "\n",
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/{model_version}\")\n",
    "\n",
    "output = model.generate(\n",
    "    [\"Happiness\", \"Дегтярев наградил мать Овечкина почетным знаком Минспорта.\"]\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffba4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получить список всех экспериментов (включая удалённые)\n",
    "all_experiments = mlflow.search_experiments(view_type=mlflow.entities.ViewType.ALL)\n",
    "\n",
    "# Вывести ID и имена\n",
    "for exp in all_experiments:\n",
    "    print(\n",
    "        f\"ID: {exp.experiment_id}, Name: {exp.name}, Lifecycle: {exp.lifecycle_stage}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb91cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.restore_experiment(experiment_id=\"522564765305824673\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564f27c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = mlflow.get_experiment_by_name(\"LoRA Fine-tuning\")\n",
    "if exp:\n",
    "    mlflow.delete_experiment(exp.experiment_id)  # Полное удаление"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
